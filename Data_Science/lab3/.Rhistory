subr.sel= which(subr.out$cp==min(subr.out$cp)); # locates minimum
subr.out$outmat; # matrix indicating which variables are included in each mod
subr.out$outmat[subr.sel,]; # row for selected model
cbind(subr.out$cp, subr.out$adjr2,
subr.out$bic, subr.out$rsq); # compare different criteria
plot(subr);
plot(subr, scale="Cp");
coef(subr,subr.sel);  # OLS coefficients
# Is the selected model different?
########################################################
###########################################################################
br = read.table("br.csv", sep = ",", header=T) # reads the data from a csv file
# We now partition our dataset in two samples.
# We randomly select a training sample of N=600 observations
Ntot = nrow(br);  # total n. of obs.
set.seed(123);    # random seed
N = 600;          # size of training sample
s = sample(1:Ntot, N);  # index of selected units
train = br[s,];   # training sample
test = br[-s,];   # validation sample
summary(train);   # data set summary
## regression model formula: full model
model_full = price~sqft+I(sqft^2)+I(sqft^3)+Age+I(Age^2)+I(sqft*Age) +
Pool+Baths+Bedrooms+Fireplace+Waterfront+
DOM+factor(Occupancy)+factor(Style);
regr_full = lm(model_full, data = train)
summary(regr_full);
############### LIBRARY LEAPS ##############################
install.packages("leaps")
library(leaps);
############### BEST SUBSET SELECTION ######################
subr = regsubsets(model_full,
data = train,
method = "exhaustive",
nbest = 1,
nvmax = 15,
force.in = 1); # all the models will contain sqft (1st variable)
# if you set it to zero, sqft will be subject to selection
# Output of best subset selection analysis
subr.out = summary(subr);   # this object will contain the subr output
subr.out$cp;                # Mallow's C_p
min(subr.out$cp);           # minimum value
subr.sel= which(subr.out$cp==min(subr.out$cp)); # locates minimum
subr.sel    # this is the index of the selected model
subr.out$outmat; # matrix indicating which variables are included in each mod
subr.out$outmat[subr.sel,]; # row for selected model
cbind(subr.out$cp, subr.out$adjr2,
subr.out$bic, subr.out$rsq); # compare different criteria: Cp, AdjR2, BIC, R2
plot(subr);
# each row is a model. The shaded rectangle indicate that
# the variable is included in model. The value of BIC is
# reported
plot(subr, scale="Cp");
#### other useful functions for retrieving info about the model fit
coef(subr,subr.sel);  # OLS coefficients
# it may be useful to list the variables selected
subr$xnames[subr.out$which[subr.sel,]]
########################################################
# Selected model (according to Cp)
model_sel = price ~ sqft+I(sqft^3)+I(Age^2)+I(sqft * Age)+Baths+
Fireplace+I(factor(Occupancy)==2)+
I(factor(Style)==2)+I(factor(Style)==4)+
I(factor(Style)==6)+I(factor(Style)==7)+
I(factor(Style)==10);
regr_sel = lm(model_sel, data = train);
summary(regr_sel);
#### analysis of selected model ---------------------------
yf = fitted(regr_sel)   # fitted values
e = residuals(regr_sel) # residuals
hist(e, 50, col = 'blue', main = 'histogram of residual')
plot(train$price,yf, col ="red") # predicted vs observed (any nonlinearity?)
plot(train$price,e, col ="red")  # residuals vs observed
abline(h=0)            # draws a horizontal line at zero
plot(yf,e, col ="red") # residuals vs predicted
#################################################################
## Validation: test sample prediction accuracy
## We use the R function predict
ypred_full = predict(regr_full, newdata = test)
pred_errors_full = test$price - ypred_full; # full model predictions for the test sample
mse_full = mean(pred_errors_full^2)   # mean square prediction error of full model
ypred_sel = predict(regr_sel, newdata = test)
pred_errors_sel = test$price - ypred_sel;   # selected model predictions
mse_sel = mean(pred_errors_sel^2)
100 * (1- mse_sel/mse_full)  # percent gain in accuracy due to model selection
############### Forward stepwise selection ######################
subr = regsubsets(model_full,
data = train,
method = "forward",
nvmax = 15,
force.in = 1); # all the models will contain sqft
# Output
subr.out = summary(subr);
subr.out$cp;
min(subr.out$cp);
subr.sel= which(subr.out$cp==min(subr.out$cp)); # locates minimum
subr.out$outmat; # matrix indicating which variables are included in each mod
subr.out$outmat[subr.sel,]; # row for selected model
cbind(subr.out$cp, subr.out$adjr2,
subr.out$bic, subr.out$rsq); # compare different criteria
plot(subr);
plot(subr, scale="Cp");
coef(subr,subr.sel);  # OLS coefficients
# Is the selected model different?
########################################################
install.packages("leaps")
install.packages("glmnet")
install.packages("leaps")
installed.packages()
N=100
N = 1000;
p = 1;
sigma = 1.8;
set.seed(123)
X=matrix(rnorm(N*p),N,p)
View(X)
View(X)
N = 1000;
p = 1;
a=0
b=40
sigma = 1.8;
set.seed(123)
X=matrix(rnorm(N*p),a,b)
View(X)
View(X)
X=matrix(rnorm(a*b),N,p)
View(X)
View(X)
N = 1000;
p = 1;
a=0
b=40
sigma = 1.8;
set.seed(123)
X=matrix(rnorm(N*p),N,p)
View(X)
View(X)
plot(X)
N = 1000;
p = 1;
a=0
b=40
sigma = 1.8;
set.seed(124)
X=matrix(rnorm(N*p),N,p)
plot(X)
N = 1000;
p = 2;
a=0
b=40
sigma = 1.8;
set.seed(123)
X=matrix(rnorm(N*p),N,p)
plot(X)
View(X)
View(X)
X=matrix(rnorm(N*p),N,b)
View(X)
View(X)
N = 1000;
p = 1;
sigma = 1.8;
set.seed(123)
X=matrix(rnorm(N*p),N,p)
plot(X)
y=0.5 * X[,410] - 2 * X[,3] + .9 * X[,123] + sigma* rnorm(N); # the true value we look for
plot(y)
y=0.5 * X[,410] - 2 * X[,3] + .9 * X[,123] + sigma* rnorm(N); # the true value we look for
y=0.5 * X[,111] - 2 * X[,3] + .9 * X[,123] + sigma* rnorm(N); # the true value we look for
y=0.5 * X[,123] - 2 + sigma* rnorm(N); # the true value we look for
??`matrix-class`\
??matrix
??vector
??vector
xval = seq(65, 85,length=100); # set range of x values for plotting
fx_0 = dnorm(xval,mean=m0,sd=s0); # Gaussian density f(x) for group 0
m0 = 80; m1 = 70;  # group means
s0 = 2; s1 = 2;    # standard deviations
xval
fx_0 = dnorm(xval,mean=m0,sd=s0); # Gaussian density f(x) for group 0
fx_1 = dnorm(xval,mean=m1,sd=s1);  # Gaussian density f(x) for group 1
N = 300;
p = 500;
N = 300;
p = 1;
sigma = 1.8;
??seed
N = 300;
p = 1;
sigma = 1.8;
random.seed(123) # to random the x
X=matrix(rnorm(N*p),N,p)
N = 300;
p = 1;
sigma = 1.8;
.Random.seed(123) # to random the x
X=matrix(rnorm(N*p),N,p)
N = 300;
p = 1;
sigma = 1.8;
.Random.seed
X=matrix(rnorm(N*p),N,p)
View(X)
N = 300;
p = 1;
sigma = 1.8;
X=.Random.seed
X1=matrix(rnorm(N*p),N,p)
X
N = 300;
p = 1;
sigma = 1.8;
X=.Random.seed[1:300]
X1=matrix(rnorm(N*p),N,p)
Y=set.seed(normal.kind = NULL)
N = 300;
p = 1;
sigma = 1.8;
X=.Random.seed[1:300]
X1=matrix(rnorm(X),N,p)
plot(X1)
N <- 10000
x <- rnbinom(N, 10, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F,
main='Positive Skewed')
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 1, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F,
main='Positive Skewed')
lines(density(x,bw=1), col='red', lwd=3)
x <- rnbinom(N, 100, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F,
main='Positive Skewed')
N <- 10000
x <- rnbinom(N, 20, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F,
main='Positive Skewed')
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F,
main='Positive Skewed')
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F)
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=T)
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab='X', ylab=' ', axes=F)
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .8)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab='X', ylab=' ', axes=F)
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .6)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab='X', ylab=' ', axes=F)
lines(density(x,bw=1), col='red', lwd=3)
y=0+0.5x
y=0+0.5x
y=[1:100]
x=[1:100]
y=0+0.5x
y=[1:100]
x=[1:100]
y=0.5x
y=range(1:100)
y
x=seq(1, 100, 1)
x=seq(1, 100, 1)
y=0.5x
x=seq(1, 100, 1)
y=seq(1, 100, 1)
y=0.5x
x=seq(1, 100, 1)
y=0.5x
x=seq(1, 100, 1)
x1=0.5x
x
y=seq(1, 100, 1)
plot(x,y)
plot(x)
plot(x, line)
t=0.1
I=100
c=(t*I)/I
c
t=0.1
I=2000
c=(t*I)/I
c
t=0.1
I=2000
c=Iˆt/I
c
t=0.1
I=2000
c=I^t/I
c
I=20008
t=0.1
I=20008
c=I^t/I
c
t=0.1
I=520008
c=I^t/I
c
x=[100 120 160 180 190 200 210 220 240 250 260 280 300 320 340 350 380 400 450 500 560 630 720 870 990 1100 1400 1900 2400 3100 3900 4700 5000 ]
x=(100, 120,160, 180, 190, 200, 210, 220, 240, 250, 260, 280, 300, 320, 340, 350, 380, 400, 450, 500, 560, 630, 720, 870, 990, 1100, 1400, 1900, 2400, 3100, 3900, 4700, 5000, 8000, 10000 )
x (100, 120,160, 180, 190, 200, 210, 220, 240, 250, 260, 280, 300, 320, 340, 350, 380, 400, 450, 500, 560, 630, 720, 870, 990, 1100, 1400, 1900, 2400, 3100, 3900, 4700, 5000, 8000, 10000 )
(2^5000)*5000
(2^3000)
l=10
k=20
a=5
b=0.3
c=1-b
a*l*kˆ(b)
> a*l*kˆb
a*l*k^b
kk=1
a*l*kk^b
kkk=0.1
a*l*kkk^b
a*l*b*k^(b-1)
> a*l*b*kk^(b-1)
a*l*b*(kk)^(b-1)
> a*l*b*(kkk)^(b-1)
a*l*b*(kk)^(b-1)
a*l*b*(kkk)^(b-1)
kkkk=0.0001
kkkk=0.0001
a*l*b*(kkkk)^(b-1)
a=2
l=10
k=20
b=0.3
k=200
kk=20000
kkk=300000000
K=20
a*l*b*(K)^(b-1)
a*l*b*(kk)^(b-1)
a*l*b*(kkk)^(b-1)
A= (1+ \alpha )
=A+B
A= (1+ aP )
B = s
A = as.matrix(data.frame(c(-1,-1),c(1,1)))
A
e <- eigen(A)
e$values
a=0.5
H=10
b = -(1+(a*H))/2
c=(4)^(0.5)
c=((1+(a*H))^2 - 4*(a*H)*(1-a))^(0.5)
a=0.5
H=10
b = -(1+(a*H))/2
c=((1+(a*H))^2 - 4*(a*H)*(1-a))^(0.5)\
a1 = 0.9
H1= 200
b1 = -(1+(a1*H))/2
c1=((1+(a1*H1))^2 - 4*(a1*H1)*(1-a1))^(0.5)
a2 = 0.1
H2 = 2
b2 = -(1+(a2*H2))/2
c2=((1+(a2*H2))^2 - 4*(a2*H2)*(1-a2))^(0.5)
a=0.5
H=10
b = -(1+(a*H))/2
c=[((1+(a*H))^2 - 4*(a*H)*(1-a))^(0.5)]/2
a1 = 0.9
H1= 200
b1 = -(1+(a1*H))/2
c1=[((1+(a1*H1))^2 - 4*(a1*H1)*(1-a1))^(0.5)]/2
a2 = 0.1
H2 = 2
b2 = -(1+(a2*H2))/2
c2=[((1+(a2*H2))^2 - 4*(a2*H2)*(1-a2))^(0.5)]/2
c2=(((1+(a2*H2))^2 - 4*(a2*H2)*(1-a2))^(0.5))/2
a=0.5
H=10
b = -(1+(a*H))/2
c=(((1+(a*H))^2 - 4*(a*H)*(1-a))^(0.5))/2
a1 = 0.9
H1= 200
b1 = -(1+(a1*H))/2
c1=(((1+(a1*H1))^2 - 4*(a1*H1)*(1-a1))^(0.5))/2
a2 = 0.1
H2 = 2
b2 = -(1+(a2*H2))/2
c2=(((1+(a2*H2))^2 - 4*(a2*H2)*(1-a2))^(0.5))/2
F=b+c
G=b-c
F1=b1+c1
G1=b1-c1
F2=b2+c2
G2=b2-c2
a2 = 0.5
H2 = 2
b2 = -(1+(a2*H2))/2
c2=(((1+(a2*H2))^2 - 4*(a2*H2)*(1-a2))^(0.5))/2
F2=b2+c2
G2=b2-c2
a1 = 0.9
H1= 20
b1 = -(1+(a1*H))/2
c1=(((1+(a1*H1))^2 - 4*(a1*H1)*(1-a1))^(0.5))/2
F1=b1+c1
G1=b1-c1
a1 = 0.9
H1= 20
b1 = -(1+(a1*H1))/2
c1=(((1+(a1*H1))^2 - 4*(a1*H1)*(1-a1))^(0.5))/2
F1=b1+c1
G1=b1-c1
setwd("~/OneDrive/Documentos/UNIVERSITY/TOR VERGATA/TAs/M - Business Statistic/lab3")
challenger = read.table("challenger.dat",head=T)
attach(challenger)
Temperature;
Damage = (NFailed > 0)
Dp = as.numeric(Damage)
Dp[14] = Dp[14]+0.05;Dp[17] = Dp[17]+0.1;Dp[23] = Dp[23]+0.05;
Dp[13] = Dp[13]-0.05;
plot(Temperature,Dp,xlab="Temperature",ylab="Damage",
col = c("red", "blue")[factor(Damage)])
lgstmodel = glm(Damage~Temperature, family=binomial(link="logit"));
summary(lgstmodel)
phat = fitted(lgstmodel)
ghat = phat > 0.5
plot(Temperature, phat,xlab="Temperature",ylab="fitted p",
col = c("red", "blue")[factor(Damage)])
abline(h=0.5)
rm(list = ls()) # remove previous objects
cdata = read.csv("german.csv", header = T);
attach(cdata);
N = nrow(cdata);
names(cdata);
# Dependent variable: Creditability:
#  1 : credit-worthy;  2 : not credit-worthy
Group = Group-1; # change the score to 0, 1.
table(Group);
table(Group, Purpose)
plot(CreditAmount,Group,xlab="CreditAmount",ylab="Credit Worthy",
col = c("red", "blue")[factor(Group)])
model_full = factor(Group)~
Duration+factor(StatusCAccount)+factor(CreditHistory)+
factor(Purpose)+CreditAmount+factor(Purpose)+
factor(SAccBonds)+factor(Employment)+factor(Instalment)+
factor(PStatusSex)+factor(OtherDebtors)+
factor(Residence)+factor(Property)+Age+
factor(OtherPlans)+factor(Housing) +NCredits+
factor(Job)+NPeople+factor(Telephone)+factor(Foreign)+
I(CreditAmount^2)+I(Duration^2)+I(Duration*CreditAmount);
full = glm(model_full,  family=binomial(link="logit"));
summary(full);
# restricted model (containing 2 explanatory variables)
model_restr = factor(Group)~Duration+CreditAmount;
restr = glm(model_restr,family=binomial(link="logit"));
summary(restr)
# stepwise selection (using the standard stat library)
sel.f = step(restr, scope=formula(full), direction="forward", k=log(N))
summary(sel.f)
sel.b = step(full, scope=formula(full), direction="backward", k=log(N))
summary(sel.b)
# the last option is for BIC instead of AIC
sel =  glm(formula(sel.f), family=binomial(link="logit"));
summary(sel)
p.hat = predict(sel, type="response"); # predicted values
g.hat = p.hat>0.5# classifier
logits = predict(sel);        # returns the linear predictor X*b
table(factor(Group), p.hat>0.5)       # confusion table
# gof
r = residuals(sel, type="pearson"); # residuals
barplot(r)
outlier = which(abs(r) == max(abs(r)));
sel$fitted.values[outlier];
sel$y[outlier];
d = residuals(sel, type="deviance");  # deviance residuals
barplot(d)
sum(d^2); # this is equal to the deviance in the summary table
plot(logits, Group, col = c("red", "blue")[factor(Group)]);
lines(logits, p.hat, type = "p", col = c("red", "blue")[factor(Group)]);
