p = 2;
a=0
b=40
sigma = 1.8;
set.seed(123)
X=matrix(rnorm(N*p),N,p)
plot(X)
View(X)
View(X)
X=matrix(rnorm(N*p),N,b)
View(X)
View(X)
N = 1000;
p = 1;
sigma = 1.8;
set.seed(123)
X=matrix(rnorm(N*p),N,p)
plot(X)
y=0.5 * X[,410] - 2 * X[,3] + .9 * X[,123] + sigma* rnorm(N); # the true value we look for
plot(y)
y=0.5 * X[,410] - 2 * X[,3] + .9 * X[,123] + sigma* rnorm(N); # the true value we look for
y=0.5 * X[,111] - 2 * X[,3] + .9 * X[,123] + sigma* rnorm(N); # the true value we look for
y=0.5 * X[,123] - 2 + sigma* rnorm(N); # the true value we look for
??`matrix-class`\
??matrix
??vector
??vector
xval = seq(65, 85,length=100); # set range of x values for plotting
fx_0 = dnorm(xval,mean=m0,sd=s0); # Gaussian density f(x) for group 0
m0 = 80; m1 = 70;  # group means
s0 = 2; s1 = 2;    # standard deviations
xval
fx_0 = dnorm(xval,mean=m0,sd=s0); # Gaussian density f(x) for group 0
fx_1 = dnorm(xval,mean=m1,sd=s1);  # Gaussian density f(x) for group 1
N = 300;
p = 500;
N = 300;
p = 1;
sigma = 1.8;
??seed
N = 300;
p = 1;
sigma = 1.8;
random.seed(123) # to random the x
X=matrix(rnorm(N*p),N,p)
N = 300;
p = 1;
sigma = 1.8;
.Random.seed(123) # to random the x
X=matrix(rnorm(N*p),N,p)
N = 300;
p = 1;
sigma = 1.8;
.Random.seed
X=matrix(rnorm(N*p),N,p)
View(X)
N = 300;
p = 1;
sigma = 1.8;
X=.Random.seed
X1=matrix(rnorm(N*p),N,p)
X
N = 300;
p = 1;
sigma = 1.8;
X=.Random.seed[1:300]
X1=matrix(rnorm(N*p),N,p)
Y=set.seed(normal.kind = NULL)
N = 300;
p = 1;
sigma = 1.8;
X=.Random.seed[1:300]
X1=matrix(rnorm(X),N,p)
plot(X1)
N <- 10000
x <- rnbinom(N, 10, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F,
main='Positive Skewed')
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 1, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F,
main='Positive Skewed')
lines(density(x,bw=1), col='red', lwd=3)
x <- rnbinom(N, 100, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F,
main='Positive Skewed')
N <- 10000
x <- rnbinom(N, 20, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F,
main='Positive Skewed')
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F,
main='Positive Skewed')
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F)
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=T)
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab='X', ylab=' ', axes=F)
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .8)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab='X', ylab=' ', axes=F)
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .6)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab='X', ylab=' ', axes=F)
lines(density(x,bw=1), col='red', lwd=3)
y=0+0.5x
y=0+0.5x
y=[1:100]
x=[1:100]
y=0+0.5x
y=[1:100]
x=[1:100]
y=0.5x
y=range(1:100)
y
x=seq(1, 100, 1)
x=seq(1, 100, 1)
y=0.5x
x=seq(1, 100, 1)
y=seq(1, 100, 1)
y=0.5x
x=seq(1, 100, 1)
y=0.5x
x=seq(1, 100, 1)
x1=0.5x
x
y=seq(1, 100, 1)
plot(x,y)
plot(x)
plot(x, line)
t=0.1
I=100
c=(t*I)/I
c
t=0.1
I=2000
c=(t*I)/I
c
t=0.1
I=2000
c=Iˆt/I
c
t=0.1
I=2000
c=I^t/I
c
I=20008
t=0.1
I=20008
c=I^t/I
c
t=0.1
I=520008
c=I^t/I
c
x=[100 120 160 180 190 200 210 220 240 250 260 280 300 320 340 350 380 400 450 500 560 630 720 870 990 1100 1400 1900 2400 3100 3900 4700 5000 ]
x=(100, 120,160, 180, 190, 200, 210, 220, 240, 250, 260, 280, 300, 320, 340, 350, 380, 400, 450, 500, 560, 630, 720, 870, 990, 1100, 1400, 1900, 2400, 3100, 3900, 4700, 5000, 8000, 10000 )
x (100, 120,160, 180, 190, 200, 210, 220, 240, 250, 260, 280, 300, 320, 340, 350, 380, 400, 450, 500, 560, 630, 720, 870, 990, 1100, 1400, 1900, 2400, 3100, 3900, 4700, 5000, 8000, 10000 )
(2^5000)*5000
(2^3000)
l=10
k=20
a=5
b=0.3
c=1-b
a*l*kˆ(b)
> a*l*kˆb
a*l*k^b
kk=1
a*l*kk^b
kkk=0.1
a*l*kkk^b
a*l*b*k^(b-1)
> a*l*b*kk^(b-1)
a*l*b*(kk)^(b-1)
> a*l*b*(kkk)^(b-1)
a*l*b*(kk)^(b-1)
a*l*b*(kkk)^(b-1)
kkkk=0.0001
kkkk=0.0001
a*l*b*(kkkk)^(b-1)
a=2
l=10
k=20
b=0.3
k=200
kk=20000
kkk=300000000
K=20
a*l*b*(K)^(b-1)
a*l*b*(kk)^(b-1)
a*l*b*(kkk)^(b-1)
A= (1+ \alpha )
=A+B
A= (1+ aP )
B = s
A = as.matrix(data.frame(c(-1,-1),c(1,1)))
A
e <- eigen(A)
e$values
a=0.5
H=10
b = -(1+(a*H))/2
c=(4)^(0.5)
c=((1+(a*H))^2 - 4*(a*H)*(1-a))^(0.5)
a=0.5
H=10
b = -(1+(a*H))/2
c=((1+(a*H))^2 - 4*(a*H)*(1-a))^(0.5)\
a1 = 0.9
H1= 200
b1 = -(1+(a1*H))/2
c1=((1+(a1*H1))^2 - 4*(a1*H1)*(1-a1))^(0.5)
a2 = 0.1
H2 = 2
b2 = -(1+(a2*H2))/2
c2=((1+(a2*H2))^2 - 4*(a2*H2)*(1-a2))^(0.5)
a=0.5
H=10
b = -(1+(a*H))/2
c=[((1+(a*H))^2 - 4*(a*H)*(1-a))^(0.5)]/2
a1 = 0.9
H1= 200
b1 = -(1+(a1*H))/2
c1=[((1+(a1*H1))^2 - 4*(a1*H1)*(1-a1))^(0.5)]/2
a2 = 0.1
H2 = 2
b2 = -(1+(a2*H2))/2
c2=[((1+(a2*H2))^2 - 4*(a2*H2)*(1-a2))^(0.5)]/2
c2=(((1+(a2*H2))^2 - 4*(a2*H2)*(1-a2))^(0.5))/2
a=0.5
H=10
b = -(1+(a*H))/2
c=(((1+(a*H))^2 - 4*(a*H)*(1-a))^(0.5))/2
a1 = 0.9
H1= 200
b1 = -(1+(a1*H))/2
c1=(((1+(a1*H1))^2 - 4*(a1*H1)*(1-a1))^(0.5))/2
a2 = 0.1
H2 = 2
b2 = -(1+(a2*H2))/2
c2=(((1+(a2*H2))^2 - 4*(a2*H2)*(1-a2))^(0.5))/2
F=b+c
G=b-c
F1=b1+c1
G1=b1-c1
F2=b2+c2
G2=b2-c2
a2 = 0.5
H2 = 2
b2 = -(1+(a2*H2))/2
c2=(((1+(a2*H2))^2 - 4*(a2*H2)*(1-a2))^(0.5))/2
F2=b2+c2
G2=b2-c2
a1 = 0.9
H1= 20
b1 = -(1+(a1*H))/2
c1=(((1+(a1*H1))^2 - 4*(a1*H1)*(1-a1))^(0.5))/2
F1=b1+c1
G1=b1-c1
a1 = 0.9
H1= 20
b1 = -(1+(a1*H1))/2
c1=(((1+(a1*H1))^2 - 4*(a1*H1)*(1-a1))^(0.5))/2
F1=b1+c1
G1=b1-c1
setwd("~/OneDrive")
setwd("~/OneDrive/Documentos/UNIVERSITY/TOR VERGATA/TAs/M - Business Statistic/lab2")
br = read.table("br.csv", sep = ",", header=T) # reads the data from a csv file
Ntot = nrow(br);  # total n. of obs.
set.seed(123);    # random seed
N = 600;          # size of training sample
s = sample(1:Ntot, N);  # index of selected units
train = br[s,];   # training sample
test = br[-s,];   # validation sample
summary(train);   # data set summary
model_full = price~sqft+I(sqft^2)+I(sqft^3)+Age+I(Age^2)+I(sqft*Age) +
Pool+Baths+Bedrooms+Fireplace+Waterfront+
DOM+factor(Occupancy)+factor(Style);
regr_full = lm(model_full, data = train)
summary(regr_full);
library(leaps);
subr = regsubsets(model_full,
data = train,
method = "exhaustive",
nbest = 1,
nvmax = 15,
force.in = 1); # all the models will contain sqft (1st variable)
# if you set it to zero, sqft will be subject to selection
subr.out = summary(subr);   # this object will contain the subr output
subr.out$cp;                # Mallow's C_p
min(subr.out$cp);           # minimum value
subr.sel= which(subr.out$cp==min(subr.out$cp)); # locates minimum
subr.sel    # this is the index of the selected model
subr.out$outmat; # matrix indicating which variables are included in each mod
subr.out$outmat[subr.sel,]; # row for selected model
cbind(subr.out$cp, subr.out$adjr2,
subr.out$bic, subr.out$rsq); # compare different criteria: Cp, AdjR2, BIC, R2
plot(subr);
plot(subr, scale="Cp");
coef(subr,subr.sel);  # OLS coefficients
subr$xnames[subr.out$which[subr.sel,]]
model_sel = price ~ sqft+I(sqft^3)+I(Age^2)+I(sqft * Age)+Baths+
Fireplace+I(factor(Occupancy)==2)+
I(factor(Style)==2)+I(factor(Style)==4)+
I(factor(Style)==6)+I(factor(Style)==7)+
I(factor(Style)==10);
regr_sel = lm(model_sel, data = train);
summary(regr_sel);
yf = fitted(regr_sel)   # fitted values
e = residuals(regr_sel) # residuals
hist(e, 50, col = 'blue', main = 'histogram of residual')
plot(train$price,yf, col ="red") # predicted vs observed (any nonlinearity?)
plot(train$price,e, col ="red")  # residuals vs observed
abline(h=0)            # draws a horizontal line at zero
plot(yf,e, col ="red") # residuals vs predicted
ypred_full = predict(regr_full, newdata = test)
pred_errors_full = test$price - ypred_full; # full model predictions for the test sample
mse_full = mean(pred_errors_full^2)   # mean square prediction error of full model
ypred_sel = predict(regr_sel, newdata = test)
pred_errors_sel = test$price - ypred_sel;   # selected model predictions
mse_sel = mean(pred_errors_sel^2)
100 * (1- mse_sel/mse_full)  # percent gain in accuracy due to model selection
subr = regsubsets(model_full,
data = train,
method = "forward",
nvmax = 15,
force.in = 1); # all the models will contain sqft
# Output
subr.out = summary(subr);
subr.out$cp;
min(subr.out$cp);
subr.sel= which(subr.out$cp==min(subr.out$cp)); # locates minimum
subr.out$outmat; # matrix indicating which variables are included in each mod
subr.out$outmat[subr.sel,]; # row for selected model
cbind(subr.out$cp, subr.out$adjr2,
subr.out$bic, subr.out$rsq); # compare different criteria
plot(subr);
plot(subr, scale="Cp");
coef(subr,subr.sel); # OLS coefficients
subr$xnames[subr.out$which[subr.sel,]]
br = read.table("br.csv", sep = ",", header=T) # reads the data from a csv file
attach(br);
# preparation of input data and output: standardization
y = scale(price) # scale to standardize the variabels in R
X = scale(data.frame(sqft, sqft^2, sqft^3, Age, Age^2, Age^3, sqft*Age, Baths, Bedrooms));
# correlation matrix
S = cor(X)
# Eigenvalues and eigenvectors of S matrix
print(S, digits = 2)
print(eigen(S), digits = 2)
# We can obtain the principal components directly
# using the princomp function:
pc.br = princomp(X);
Z = pc.br$scores; # principal components scores
pairs(Z[,1:4]);   # scatterplot of the first 4 pc's
plot(pc.br);      # bar plot of the eigenvalues (screeplot)
pc.br$sdev        # Standard deviation of the pc's
# same as sqrt of the eigenvalues:
sqrt(eigen(S)$values)
print(var(Z), digits=5);   # pc's are orthogonal and have decresing variances
p = ncol(Z)
N = nrow(Z)
AIC = rep("NaN",p)
for (i in 1:p)
{
pcr = lm(y~-1+Z[,1:i]);
print(summary(pcr));
AIC[i] = 2*log(summary(e^2)/N)+2*i/N;
BIC[i] = 2*log(summary(e^2)/N)+log(N)*i/N;
}
plot(1:p, AIC)
pcregr = lm(y~-1+Z);
summary(pcregr);
summary(lm(y~-1+X));
# would you support a strategy that includes only the first
# m pc's?
# Later on we will introduce the following useful graph:
biplot(princomp(X), cex = 0.6, col=c(2,4));
setwd("~/OneDrive/Documentos/UNIVERSITY/TOR VERGATA/TAs/M - Business Statistic/lab2")
br = read.table("br.csv", sep = ",", header=T) # reads the data from a csv file
attach(br);
# preparation of input data and output: standardization
y = scale(price) # scale to standardize the variabels in R
X = scale(data.frame(sqft, sqft^2, sqft^3, Age, Age^2, Age^3, sqft*Age, Baths, Bedrooms));
# correlation matrix
S = cor(X)
# Eigenvalues and eigenvectors of S matrix
print(S, digits = 2)
print(eigen(S), digits = 2)
pc.br = princomp(X);
Z = pc.br$scores; # principal components scores
pairs(Z[,1:4]);   # scatterplot of the first 4 pc's
plot(pc.br);      # bar plot of the eigenvalues (screeplot)
pc.br$sdev        # Standard deviation of the pc's
# same as sqrt of the eigenvalues:
sqrt(eigen(S)$values)
print(var(Z), digits=5);   # pc's are orthogonal and have decresing variances
p = ncol(Z)
N = nrow(Z)
AIC = rep("NaN",p)
for (i in 1:p)
{
pcr = lm(y~-1+Z[,1:i]);
print(summary(pcr));
AIC[i] = 2*log(summary(e^2)/N)+2*i/N;
BIC[i] = 2*log(summary(e^2)/N)+log(N)*i/N;
}
plot(1:p, AIC)
###########################################################################
br = read.table("br.csv", sep = ",", header=T) # reads the data from a csv file
attach(br);
# preparation of input data and output: standardization
y = scale(price)
X = scale(data.frame(sqft, sqft^2, sqft^3, Age, Age^2, Age^3, sqft*Age, Baths, Bedrooms));
# correlation matrix
S = cor(X)
# Eigenvalues and eigenvectors of S matrix
print(S, digits = 2)
print(eigen(S), digits = 2)
####################################################################
# We can obtain the principal components directly
# using the princomp function:
pc.br = princomp(X);
Z = pc.br$scores; # principal components scores
pairs(Z[,1:4]);   # scatterplot of the first 4 pc's
plot(pc.br);      # bar plot of the eigenvalues (screeplot)
pc.br$sdev        # Standard deviation of the pc's
# same as sqrt of the eigenvalues:
sqrt(eigen(S)$values)
print(var(Z), digits=5);   # pc's are orthogonal and have decresing variances
# creates a matrix with zero covariances and eigenvalues as variances
p = ncol(Z)
N = nrow(Z)
AIC = rep("NaN",p)
for (i in 1:p)
{
pcr = lm(y~-1+Z[,1:i]);
print(summary(pcr));
AIC[i] = 2*log(summary(pcr)$sigma)+2*i/N;
}
plot(1:p, AIC)
pcregr = lm(y~-1+Z);
summary(pcregr);
summary(lm(y~-1+X));
summary(lm(y~X));
biplot(princomp(X), cex = 0.6);
library(glmnet);
?glmnet
br = read.table("br.csv", sep = ",", header=T) # reads the data from a csv file
attach(br);
# We randomly select a training sample of N=600 observations
Ntot = nrow(br);  # total n. of obs.
set.seed(401);    # random seed
N = 600;          # size of training sample
s = sample(1:Ntot, N);  # index of selected units
# partition the dataset
y.train = price[s];
y.test = price[-s];
Occ = model.matrix(~factor(Occupancy))[,2:3]; # dummies for occupancy
Sty = model.matrix(~factor(Style))[,2:10]; # dummies for occupancy
X = data.frame(sqft, sqft^2, Age, Age^2, sqft*Age, Pool, Bedrooms,Fireplace,Waterfront, Occ, Sty );
X.train = as.matrix(X[s,]);
X.test = as.matrix(X[-s,]);
head(X.test)
fit.lasso = glmnet(X.train, y.train, family="gaussian")
plot(fit.lasso,label=TRUE)
plot(fit.lasso,xvar = "lambda", label=TRUE)
fit.lasso
cv.fit=cv.glmnet(X.train, y.train, nfolds=20)
plot(cv.fit)
coef(cv.fit, s="lambda.min")
y.test.lasso = predict(cv.fit, newx=X.test, s="lambda.min")
y.test.full = predict(cv.fit, newx=X.test, s=0)
pairs(cbind(y.test, y.test.lasso, y.test.full) )
pred.errors.lasso = y.test - y.test.lasso;   # selected model predictions
pred.errors.full = y.test - y.test.full;   # selected model predictions
mse.lasso = mean(pred.errors.lasso^2)
mse.full = mean(pred.errors.full^2)
100 * (1- mse.lasso/mse.full)  # percent gain in accuracy due to shrinkage
