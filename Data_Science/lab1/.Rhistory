100 * (1- mse_sel/mse_full)  # percent gain in accuracy due to model selection
############### Forward stepwise selection ######################
subr = regsubsets(model_full,
data = train,
method = "forward",
nvmax = 15,
force.in = 1); # all the models will contain sqft
# Output
subr.out = summary(subr);
subr.out$cp;
min(subr.out$cp);
subr.sel= which(subr.out$cp==min(subr.out$cp)); # locates minimum
subr.out$outmat; # matrix indicating which variables are included in each mod
subr.out$outmat[subr.sel,]; # row for selected model
cbind(subr.out$cp, subr.out$adjr2,
subr.out$bic, subr.out$rsq); # compare different criteria
plot(subr);
plot(subr, scale="Cp");
coef(subr,subr.sel);  # OLS coefficients
# Is the selected model different?
########################################################
###########################################################################
br = read.table("br.csv", sep = ",", header=T) # reads the data from a csv file
# We now partition our dataset in two samples.
# We randomly select a training sample of N=600 observations
Ntot = nrow(br);  # total n. of obs.
set.seed(123);    # random seed
N = 600;          # size of training sample
s = sample(1:Ntot, N);  # index of selected units
train = br[s,];   # training sample
test = br[-s,];   # validation sample
summary(train);   # data set summary
## regression model formula: full model
model_full = price~sqft+I(sqft^2)+I(sqft^3)+Age+I(Age^2)+I(sqft*Age) +
Pool+Baths+Bedrooms+Fireplace+Waterfront+
DOM+factor(Occupancy)+factor(Style);
regr_full = lm(model_full, data = train)
summary(regr_full);
############### LIBRARY LEAPS ##############################
install.packages("leaps")
library(leaps);
############### BEST SUBSET SELECTION ######################
subr = regsubsets(model_full,
data = train,
method = "exhaustive",
nbest = 1,
nvmax = 15,
force.in = 1); # all the models will contain sqft (1st variable)
# if you set it to zero, sqft will be subject to selection
# Output of best subset selection analysis
subr.out = summary(subr);   # this object will contain the subr output
subr.out$cp;                # Mallow's C_p
min(subr.out$cp);           # minimum value
subr.sel= which(subr.out$cp==min(subr.out$cp)); # locates minimum
subr.sel    # this is the index of the selected model
subr.out$outmat; # matrix indicating which variables are included in each mod
subr.out$outmat[subr.sel,]; # row for selected model
cbind(subr.out$cp, subr.out$adjr2,
subr.out$bic, subr.out$rsq); # compare different criteria: Cp, AdjR2, BIC, R2
plot(subr);
# each row is a model. The shaded rectangle indicate that
# the variable is included in model. The value of BIC is
# reported
plot(subr, scale="Cp");
#### other useful functions for retrieving info about the model fit
coef(subr,subr.sel);  # OLS coefficients
# it may be useful to list the variables selected
subr$xnames[subr.out$which[subr.sel,]]
########################################################
# Selected model (according to Cp)
model_sel = price ~ sqft+I(sqft^3)+I(Age^2)+I(sqft * Age)+Baths+
Fireplace+I(factor(Occupancy)==2)+
I(factor(Style)==2)+I(factor(Style)==4)+
I(factor(Style)==6)+I(factor(Style)==7)+
I(factor(Style)==10);
regr_sel = lm(model_sel, data = train);
summary(regr_sel);
#### analysis of selected model ---------------------------
yf = fitted(regr_sel)   # fitted values
e = residuals(regr_sel) # residuals
hist(e, 50, col = 'blue', main = 'histogram of residual')
plot(train$price,yf, col ="red") # predicted vs observed (any nonlinearity?)
plot(train$price,e, col ="red")  # residuals vs observed
abline(h=0)            # draws a horizontal line at zero
plot(yf,e, col ="red") # residuals vs predicted
#################################################################
## Validation: test sample prediction accuracy
## We use the R function predict
ypred_full = predict(regr_full, newdata = test)
pred_errors_full = test$price - ypred_full; # full model predictions for the test sample
mse_full = mean(pred_errors_full^2)   # mean square prediction error of full model
ypred_sel = predict(regr_sel, newdata = test)
pred_errors_sel = test$price - ypred_sel;   # selected model predictions
mse_sel = mean(pred_errors_sel^2)
100 * (1- mse_sel/mse_full)  # percent gain in accuracy due to model selection
############### Forward stepwise selection ######################
subr = regsubsets(model_full,
data = train,
method = "forward",
nvmax = 15,
force.in = 1); # all the models will contain sqft
# Output
subr.out = summary(subr);
subr.out$cp;
min(subr.out$cp);
subr.sel= which(subr.out$cp==min(subr.out$cp)); # locates minimum
subr.out$outmat; # matrix indicating which variables are included in each mod
subr.out$outmat[subr.sel,]; # row for selected model
cbind(subr.out$cp, subr.out$adjr2,
subr.out$bic, subr.out$rsq); # compare different criteria
plot(subr);
plot(subr, scale="Cp");
coef(subr,subr.sel);  # OLS coefficients
# Is the selected model different?
########################################################
install.packages("leaps")
install.packages("glmnet")
install.packages("leaps")
installed.packages()
N=100
N = 1000;
p = 1;
sigma = 1.8;
set.seed(123)
X=matrix(rnorm(N*p),N,p)
View(X)
View(X)
N = 1000;
p = 1;
a=0
b=40
sigma = 1.8;
set.seed(123)
X=matrix(rnorm(N*p),a,b)
View(X)
View(X)
X=matrix(rnorm(a*b),N,p)
View(X)
View(X)
N = 1000;
p = 1;
a=0
b=40
sigma = 1.8;
set.seed(123)
X=matrix(rnorm(N*p),N,p)
View(X)
View(X)
plot(X)
N = 1000;
p = 1;
a=0
b=40
sigma = 1.8;
set.seed(124)
X=matrix(rnorm(N*p),N,p)
plot(X)
N = 1000;
p = 2;
a=0
b=40
sigma = 1.8;
set.seed(123)
X=matrix(rnorm(N*p),N,p)
plot(X)
View(X)
View(X)
X=matrix(rnorm(N*p),N,b)
View(X)
View(X)
N = 1000;
p = 1;
sigma = 1.8;
set.seed(123)
X=matrix(rnorm(N*p),N,p)
plot(X)
y=0.5 * X[,410] - 2 * X[,3] + .9 * X[,123] + sigma* rnorm(N); # the true value we look for
plot(y)
y=0.5 * X[,410] - 2 * X[,3] + .9 * X[,123] + sigma* rnorm(N); # the true value we look for
y=0.5 * X[,111] - 2 * X[,3] + .9 * X[,123] + sigma* rnorm(N); # the true value we look for
y=0.5 * X[,123] - 2 + sigma* rnorm(N); # the true value we look for
??`matrix-class`\
??matrix
??vector
??vector
xval = seq(65, 85,length=100); # set range of x values for plotting
fx_0 = dnorm(xval,mean=m0,sd=s0); # Gaussian density f(x) for group 0
m0 = 80; m1 = 70;  # group means
s0 = 2; s1 = 2;    # standard deviations
xval
fx_0 = dnorm(xval,mean=m0,sd=s0); # Gaussian density f(x) for group 0
fx_1 = dnorm(xval,mean=m1,sd=s1);  # Gaussian density f(x) for group 1
N = 300;
p = 500;
N = 300;
p = 1;
sigma = 1.8;
??seed
N = 300;
p = 1;
sigma = 1.8;
random.seed(123) # to random the x
X=matrix(rnorm(N*p),N,p)
N = 300;
p = 1;
sigma = 1.8;
.Random.seed(123) # to random the x
X=matrix(rnorm(N*p),N,p)
N = 300;
p = 1;
sigma = 1.8;
.Random.seed
X=matrix(rnorm(N*p),N,p)
View(X)
N = 300;
p = 1;
sigma = 1.8;
X=.Random.seed
X1=matrix(rnorm(N*p),N,p)
X
N = 300;
p = 1;
sigma = 1.8;
X=.Random.seed[1:300]
X1=matrix(rnorm(N*p),N,p)
Y=set.seed(normal.kind = NULL)
N = 300;
p = 1;
sigma = 1.8;
X=.Random.seed[1:300]
X1=matrix(rnorm(X),N,p)
plot(X1)
N <- 10000
x <- rnbinom(N, 10, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F,
main='Positive Skewed')
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 1, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F,
main='Positive Skewed')
lines(density(x,bw=1), col='red', lwd=3)
x <- rnbinom(N, 100, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F,
main='Positive Skewed')
N <- 10000
x <- rnbinom(N, 20, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F,
main='Positive Skewed')
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F,
main='Positive Skewed')
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=F)
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab=' ', ylab=' ', axes=T)
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .5)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab='X', ylab=' ', axes=F)
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .8)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab='X', ylab=' ', axes=F)
lines(density(x,bw=1), col='red', lwd=3)
N <- 10000
x <- rnbinom(N, 10, .6)
hist(x,
xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1,
col='lightblue', xlab='X', ylab=' ', axes=F)
lines(density(x,bw=1), col='red', lwd=3)
y=0+0.5x
y=0+0.5x
y=[1:100]
x=[1:100]
y=0+0.5x
y=[1:100]
x=[1:100]
y=0.5x
y=range(1:100)
y
x=seq(1, 100, 1)
x=seq(1, 100, 1)
y=0.5x
x=seq(1, 100, 1)
y=seq(1, 100, 1)
y=0.5x
x=seq(1, 100, 1)
y=0.5x
x=seq(1, 100, 1)
x1=0.5x
x
y=seq(1, 100, 1)
plot(x,y)
plot(x)
plot(x, line)
t=0.1
I=100
c=(t*I)/I
c
t=0.1
I=2000
c=(t*I)/I
c
t=0.1
I=2000
c=Iˆt/I
c
t=0.1
I=2000
c=I^t/I
c
I=20008
t=0.1
I=20008
c=I^t/I
c
t=0.1
I=520008
c=I^t/I
c
x=[100 120 160 180 190 200 210 220 240 250 260 280 300 320 340 350 380 400 450 500 560 630 720 870 990 1100 1400 1900 2400 3100 3900 4700 5000 ]
x=(100, 120,160, 180, 190, 200, 210, 220, 240, 250, 260, 280, 300, 320, 340, 350, 380, 400, 450, 500, 560, 630, 720, 870, 990, 1100, 1400, 1900, 2400, 3100, 3900, 4700, 5000, 8000, 10000 )
x (100, 120,160, 180, 190, 200, 210, 220, 240, 250, 260, 280, 300, 320, 340, 350, 380, 400, 450, 500, 560, 630, 720, 870, 990, 1100, 1400, 1900, 2400, 3100, 3900, 4700, 5000, 8000, 10000 )
(2^5000)*5000
(2^3000)
l=10
k=20
a=5
b=0.3
c=1-b
a*l*kˆ(b)
> a*l*kˆb
a*l*k^b
kk=1
a*l*kk^b
kkk=0.1
a*l*kkk^b
a*l*b*k^(b-1)
> a*l*b*kk^(b-1)
a*l*b*(kk)^(b-1)
> a*l*b*(kkk)^(b-1)
a*l*b*(kk)^(b-1)
a*l*b*(kkk)^(b-1)
kkkk=0.0001
kkkk=0.0001
a*l*b*(kkkk)^(b-1)
a=2
l=10
k=20
b=0.3
k=200
kk=20000
kkk=300000000
K=20
a*l*b*(K)^(b-1)
a*l*b*(kk)^(b-1)
a*l*b*(kkk)^(b-1)
A= (1+ \alpha )
=A+B
A= (1+ aP )
B = s
A = as.matrix(data.frame(c(-1,-1),c(1,1)))
A
e <- eigen(A)
e$values
a=0.5
H=10
b = -(1+(a*H))/2
c=(4)^(0.5)
c=((1+(a*H))^2 - 4*(a*H)*(1-a))^(0.5)
a=0.5
H=10
b = -(1+(a*H))/2
c=((1+(a*H))^2 - 4*(a*H)*(1-a))^(0.5)\
a1 = 0.9
H1= 200
b1 = -(1+(a1*H))/2
c1=((1+(a1*H1))^2 - 4*(a1*H1)*(1-a1))^(0.5)
a2 = 0.1
H2 = 2
b2 = -(1+(a2*H2))/2
c2=((1+(a2*H2))^2 - 4*(a2*H2)*(1-a2))^(0.5)
a=0.5
H=10
b = -(1+(a*H))/2
c=[((1+(a*H))^2 - 4*(a*H)*(1-a))^(0.5)]/2
a1 = 0.9
H1= 200
b1 = -(1+(a1*H))/2
c1=[((1+(a1*H1))^2 - 4*(a1*H1)*(1-a1))^(0.5)]/2
a2 = 0.1
H2 = 2
b2 = -(1+(a2*H2))/2
c2=[((1+(a2*H2))^2 - 4*(a2*H2)*(1-a2))^(0.5)]/2
c2=(((1+(a2*H2))^2 - 4*(a2*H2)*(1-a2))^(0.5))/2
a=0.5
H=10
b = -(1+(a*H))/2
c=(((1+(a*H))^2 - 4*(a*H)*(1-a))^(0.5))/2
a1 = 0.9
H1= 200
b1 = -(1+(a1*H))/2
c1=(((1+(a1*H1))^2 - 4*(a1*H1)*(1-a1))^(0.5))/2
a2 = 0.1
H2 = 2
b2 = -(1+(a2*H2))/2
c2=(((1+(a2*H2))^2 - 4*(a2*H2)*(1-a2))^(0.5))/2
F=b+c
G=b-c
F1=b1+c1
G1=b1-c1
F2=b2+c2
G2=b2-c2
a2 = 0.5
H2 = 2
b2 = -(1+(a2*H2))/2
c2=(((1+(a2*H2))^2 - 4*(a2*H2)*(1-a2))^(0.5))/2
F2=b2+c2
G2=b2-c2
a1 = 0.9
H1= 20
b1 = -(1+(a1*H))/2
c1=(((1+(a1*H1))^2 - 4*(a1*H1)*(1-a1))^(0.5))/2
F1=b1+c1
G1=b1-c1
a1 = 0.9
H1= 20
b1 = -(1+(a1*H1))/2
c1=(((1+(a1*H1))^2 - 4*(a1*H1)*(1-a1))^(0.5))/2
F1=b1+c1
G1=b1-c1
setwd("~/OneDrive/Documentos/UNIVERSITY/TOR VERGATA/TAs/M - Business Statistic/lab1")
br = read.table("br.csv", sep = ",", header=T) # reads the data from a csv file
attach(br)
hist(price, 30, col = "blue")      # Histogram
hist(log(price), 30, col = "red")  # Histogram of log transformation
plot(sqft,price, col ="red") # Scatterplot of prices vs sqft
plot(Age, price, col ="red") # Scatterplot of prices vs age (note: age is discrete)
plot( factor(Occupancy), price, col ='red') # Boxplots by Occupancy
plot( factor(Style), price, col='yellow')
## regression model
regr = lm(price~sqft+Age+Pool+Bedrooms+Fireplace+Waterfront+DOM)
summary(regr)
coefficients(regr)  # OLS coefficients
CovBeta = vcov(regr) # covariance matrix for model parameters
CovBeta[2,2] # variance of beta_1_hat
sqrt(CovBeta[2,2]) # std err of beta_1_hat (compare with table)
coefficients(regr)[2]/sqrt(CovBeta[2,2]) # t statistic
confint(regr, level=0.95)
yf = fitted(regr)   # fitted values
e = residuals(regr) # residuals
mean(e)             # a property of the residuals: zero mean
hist(e, 60, col = 'blue', main = ' ') # do we observe anything peculiar?
cor(yf,e)     # as we know, residuals are uncorrelated with fitted values
cor(sqft,e)   # and each of the explanatory variables
plot(price,yf, col ="red") # predicted vs observed (any nonlinearity?)
plot(price,e, col ="red")  # residuals vs observed
abline(h=0)            # draws a horizontal line at zero
plot(yf,e, col ="red") # residuals vs predicted to check variance
abline(h=0)
# note: there is evidence for misspecification of functional form
# diagnostic plots produced automatically by lm
################################
h = hatvalues(regr) # diagonal elements of hat matrix
sum(h) # this is trace(H)=p+1
mean(h)
barplot(h, col = 'blue')
abline(h=mean(h), col = 'red')
r = rstandard(regr) #
barplot(h, col = 'blue')
plot(price,e, col ="red")  # residuals vs observed
hist(e, 60, col = 'blue', main = ' ') # do we observe anything peculiar?
cor(yf,e)     # as we know, residuals are uncorrelated with fitted values
cor(sqft,e)   # and each of the explanatory variables
plot(price,yf, col ="red") # predicted vs observed (any nonlinearity?)
plot(price,e, col ="red")  # residuals vs observed
abline(h=0)            # draws a horizontal line at zero
plot(yf,e, col ="red") # residuals vs predicted to check variance
abline(h=0)
h = hatvalues(regr) # diagonal elements of hat matrix
sum(h) # this is trace(H)=p+1
mean(h)
barplot(h, col = 'blue')
abline(h=mean(h), col = 'red')
r = rstandard(regr) # standardized residuals
plot(yf,r)
plot(h,r)
